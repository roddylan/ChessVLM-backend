{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from constants import STARTING_FEN \n",
    "import chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using deepseek distilled r1 (deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"microsoft/phi-2\",\n",
    "    # huggingfacehub_api_token=\"\",\n",
    "    task=\"text-generation\",\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "chat = ChatHuggingFace(llm=llm, verbose=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"fen\", \"player\", \"bot\"],\n",
    "    template='''We are playing a chess game (me vs you). I am {player}, and you are {bot}. It is your turn to move. Respond with your move in standard chess notation (e.g., 'e4') followed by the FULL FEN string after your move. The current FEN is: \"{fen}\"\n",
    "    Format your response exactly as follows:\n",
    "    Move: [your move]\n",
    "    FEN: [resulting FEN string]\n",
    "    '''\n",
    "    # template=\"We are playing a chess game. I am {player}, you are {bot}. Make a move and respond with the FULL FEN string resulting from your move. The current FEN is {fen}\"\n",
    ")\n",
    "\n",
    "invoked = template.invoke(\n",
    "    {\"fen\": f\"{STARTING_FEN}\",\n",
    "     \"player\": \"black\",\n",
    "     \"bot\": \"white\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are playing a chess game (me vs you). I am black, and you are white. It is your turn to move. Respond with your move in standard chess notation (e.g., 'e4') followed by the FULL FEN string after your move. The current FEN is: \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\"\n",
      "    Format your response exactly as follows:\n",
      "    Move: [your move]\n",
      "    FEN: [resulting FEN string]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(invoked.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq e3 0 1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test='''\n",
    "Move: e4\n",
    "FEN: rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq e3 0 1\n",
    "'''\n",
    "test.split('FEN: ')[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodericklan/Projects/ChessVLM-backend/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model unavailable\n",
      "503 Server Error: Service Temporarily Unavailable for url: https://router.huggingface.co/hf-inference/models/microsoft/phi-2\n",
      "<class 'huggingface_hub.errors.HfHubHTTPError'>\n"
     ]
    }
   ],
   "source": [
    "cc = None\n",
    "try:\n",
    "    resp = llm.invoke(invoked)\n",
    "    valid = chess.Board(resp.split('FEN: ')[-1].strip()).is_valid()\n",
    "    print(resp)\n",
    "except Exception as e:\n",
    "    print(f\"Model unavailable\\n{e}\")\n",
    "    print(type(e))\n",
    "    cc = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\"\n",
    "\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
